# ELECTRA: PyTorch Reimplementation

This repository contains a complete PyTorch reimplementation of the ELECTRA model  
as described in the ICLR 2020 paper:

> **ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators**  
> Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning  
> [Paper Link](https://arxiv.org/abs/2003.10555)

---

## ğŸ“ Project Objective

- Reproduce the ELECTRA-Small model from scratch using **only PyTorch**
- Train on small-scale corpus (e.g. OpenWebText or Wiki subset)
- Fine-tune on selected **GLUE tasks**
- Propose and test our own improvements

---

## ğŸ§  Model Architecture

### ELECTRA Overview

The ELECTRA architecture consists of:

1. **Generator**  
   - Small BERT-style masked language model  
   - Predicts masked tokens  
   - Only used to corrupt original inputs

2. **Discriminator**  
   - Binary classifier (real or replaced)  
   - Trained on **all tokens** (higher sample efficiency)

### Architecture Diagram

![ELECTRA Architecture](./images/project_steps_en.png)

---

## ğŸ—‚ï¸ Project Structure

electra-project/
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ generator.py # Generator implementation (MLM-style BERT)
â”‚ â”œâ”€â”€ discriminator.py # Discriminator for replaced token detection
â”œâ”€â”€ data/
â”‚ â””â”€â”€ dataset.py # Token replacement dataset
â”œâ”€â”€ train/
â”‚ â”œâ”€â”€ pretrain.py # Training loop for ELECTRA pretraining
â”‚ â””â”€â”€ finetune.py # Fine-tuning on GLUE tasks
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ project_steps_en.png
â”‚ â””â”€â”€ project_steps_ko.png
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ tokenization.py # BERT tokenizer or WordPiece utilities
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ electra_small_config.json
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

yaml
ë³µì‚¬
í¸ì§‘

---

## âš™ï¸ Training Workflow

```text
1. Raw text corpus
     â†“
2. Apply token masking
     â†“
3. Generator predicts masked tokens
     â†“
4. Replaced input created using generator output
     â†“
5. Discriminator predicts real/fake for each token
     â†“
6. Binary classification loss used to update model
ğŸ§ª Evaluation Plan
Fine-tune on:

SST-2 (Sentiment Classification)

RTE or CoLA

Compare with original ELECTRA-Small benchmarks

Propose minor architectural changes (e.g. dropout, reduced heads)

ğŸ‘¥ Team Members
Member A: Model architecture & training implementation

Member B: Data preprocessing, token replacement logic

Member C: Evaluation, visualization, fine-tuning analysis

ğŸ“… Project Timeline
April 17: Paper selection âœ…

May: Implementation & training ğŸ› ï¸

June 13: Final presentation ğŸ§‘â€ğŸ«

June 24: Final report & GitHub submission ğŸ“